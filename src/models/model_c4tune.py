"""

C4TUNE

"""

import torch
from torch import nn


class EnvInputBranch(nn.Module):
    '''
    
    Generates encoding for A/CO2 or A/light curve from the X (CO2 or light) and 
    Y (Anet) values and the constant light or CO2 values.
    The concatenated input is processed by an LSTM module and the final curve 
    encoding the generated by an additional linear layer following the LSTM
    layer.
    
    '''
    
    def __init__(self, input_dim, hidden_dim, encoding_dim):
        '''
        Initialization of EnvInputBranch
    
        Parameters
        ----------
        input_dim : int
            Dimension of curve input. This is 2*<curve length> + 1, because it 
            contains the X and Y values of the curve as well as the respective
            constant value of light or CO2.
        hidden_dim : int
            Dimension of hidden layers in curve encoding LSTM module.
        encoding_dim : int
            Dimension of final curve encoding.

        '''
        
        super(EnvInputBranch, self).__init__()
        
        # LSTM module
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        
        # Create curve encoding from final LSTM hidden state
        self.env_output = nn.Linear(hidden_dim, encoding_dim)
        self.relu = nn.ReLU()
        
    def forward(self, curve_input):
        '''
        Forward pass through curve processing branch.

        Parameters
        ----------
        curve_input : Tensor
            Concatenated A/CO2 or A/light curve input.

        '''
        
        # Get final hidden state from LSTM module
        _, (h_final, _) = self.lstm(curve_input)
        h_final = h_final.squeeze(0)  # remove first dimension
        
        # Create curve encoding
        output = self.env_output(h_final)
        
        return self.relu(output)

class ParameterPredictionModel(nn.Module):    
    '''
    This model has two input branches for A/CO2 and A/light curves and associated
    experimental conditions, respectively. The two different inputs are processed 
    using LSTM modules, which produce encodings of the curve inputs. The two
    encodings are then combined and further processed by two dense layers, each 
    followed by ReLU activation. The output from the final layer, x, is transformed 
    by exp(Lx), where L is the Cholesky decomposition of the covariance matrix of
    the logarithmized parameter values (Sigma(log(P)) = L^T*L).
    To obtain parameters that can be used in the C4 photosynthesis ODE model, the 
    predicted values, f, must be multiplied by the average parameter values.
    By doing this, parameter sets are created that come from a distribution that
    has the same covariance structure as the original parameters in the dataset:
    P* = exp(L*f) + exp(log(mean(P))) = exp(L*f) + mean(P)
    This massively increases the probability that an ODE model parameterized with
    the predicted parameters results in meaningful A/CO2 and A/light curves.
    
    Args:
        model_config (OmegaConf)
        
    model_config fields:
        n_co2_steps : int
            Number of CO2 steps in A/CO2 response curve.
        n_light_steps : int
            Number of light steps in A/light response curve.
        n_params : int
            Number of parameters to be predicted by the model.
        curve_encoder_hidden_dim : int
            Dimension of hidden layers in curve encoding LSTM module.
        curve_encoding_dim : int
            Dimension of final curve encoding.
        curve_fusion_dim : int
            Dimension of (potentially) lower dimensional represetation of the
            combined curve encodings.
        param_pred_hidden_dim_1 : int
            Dimension of the first hidden layer for the prediction of parameter
            deviations.
        param_pred_hidden_dim_2 : int
            Dimension of the second hidden layer for the prediction of parameter
            deviations.
        weight_init : str
            Type of weight initialization. Currently supported are "Xavier" and 
            "Kaiming".
        
    '''
    
    def __init__(self, model_config, L=None):
        '''

        Parameters
        ----------
        model_config : OmegaConf
            model parameters.
        L : Tensor or None
            Cholesky decomposition of the covariance matrix of log-transformed 
            parameters. The default is None

        '''
        

        super(ParameterPredictionModel, self).__init__()
        
        # weight initialization type
        self.weight_init = model_config.weight_init
        
        # Cholesky decomposition
        self._L = L
        
        # input encoder branches
        self.aci_input_branch = EnvInputBranch(
            2*model_config.n_co2_steps+1,
            model_config.curve_encoder_hidden_dim,
            model_config.curve_encoding_dim
        )
        self.aq_input_branch = EnvInputBranch(
            2*model_config.n_light_steps+1,
            model_config.curve_encoder_hidden_dim,
            model_config.curve_encoding_dim)
        
        # Linear layer to combine curve encodings for parameter prediction
        self.curve_encoding_comb = nn.Sequential(
            nn.Linear(
                2*model_config.curve_encoding_dim,
                model_config.curve_fusion_dim),
            nn.ReLU()
            )
        
        # parameter prediction from combined encoding
        self.param_predictor = nn.Sequential(
            nn.Linear(
                model_config.curve_fusion_dim,
                model_config.param_pred_hidden_dim_1),
            nn.ReLU(),
            nn.Linear(
                model_config.param_pred_hidden_dim_1,
                model_config.param_pred_hidden_dim_2),
            nn.ReLU(),
            nn.Linear(
                model_config.param_pred_hidden_dim_2,
                model_config.n_params),
            )
        
        # initialize weights
        self.apply(self.init_weights)
    
    @property
    def L(self):
        return self._L
    
    @L.setter
    def L(self, L):
        self._L = L.unsqueeze(0)
        
    
    def forward(self, a_co2_curve, a_light_curve):
        '''
        Forward pass through parameter prediction neural network.

        Parameters
        ----------
        a_co2_curve : Tensor
            Concatenated and standardized A/CO2 curves and respective CO2 steps
            as well as constant light value
        a_light_curve : Tensor
            Concatenated and standardized A/light curves and respective light
            steps as well as constant CO2 value

        Returns
        -------
        output : Tensor
            predicted parameter deviations

        '''
           
        # generate curve encoding
        a_co2_curve_encoding = self.aci_input_branch(a_co2_curve)
        a_light_curve_encoding = self.aq_input_branch(a_light_curve)
        
        # combine curve encodings
        combined_curve_encodings = self.curve_encoding_comb(
            torch.cat((a_co2_curve_encoding, a_light_curve_encoding), dim = -1))
        
        # predict raw parameter deviations from combined curve encodings
        param_prediction = self.param_predictor(combined_curve_encodings)
        
        # apply Cholesky decomposition
        output = self.apply_cholesky(param_prediction)
        
        return output
    
    def init_weights(self, m):
        '''
        Initialization of model weights and biases.

        Parameters
        ----------
        m : NeuralNetwork (nn.Module)
            neural network

        '''
        
        if isinstance(m, nn.Linear):
            # weights
            if self.weight_init == "Kaiming":
                torch.nn.init.kaiming_uniform_(m.weight)
            elif self.weight_init == "Xavier":
                torch.nn.init.xavier_uniform_(m.weight)
            elif self.weight_init == None:
                pass
            else:
                print("Unknown initialization function.")
                
            # biases
            m.bias.data.fill_(0.01)
    
    def apply_cholesky(self, param_prediction):
        '''
        Apply the structure of valid parameter sets to predicted parameters
        to increase the probability that they can be directly used in the ODE
        model to simulate A/CO2 and A/light curves.
        The transformed parameter deviations are exponentiated because they are
        expected to originate from a log-normal distribution to ensure positivity
        of the predicted values.

        Parameters
        ----------
        param_prediction : Tensor
            raw parameter deviation predictions from final neural network layer

        Returns
        -------
        output : Tensor
            Transformed parameter deviations

        '''

        if self._L != None:
            L = self._L.repeat(param_prediction.shape[0], 1, 1)
            output = torch.exp(
                L.matmul(param_prediction.unsqueeze(2)).squeeze(2)
                )

        else:
            output = param_prediction
            
        return output
        
        
        
        
        